Hive is a data warehouse software enables reading, writing, and managing large datasets in distributed storage. It process structured and semi-structured data in Hadoop.

Hive provides standard SQL functionality, Hive's SQL can also be extended with user code via 
				user defined functions (UDFs),
				user defined aggregates (UDAFs), 
         and user defined table functions (UDTFs).

Hive is not designed for online transaction processing (OLTP) workloads.
It is best used for traditional data warehousing tasks. Hive comes with built in connectors for comma and tab-separated values (CSV/TSV) text files, 
Apache Parquet™, Apache ORC™, and other formats. Hive is not designed for online transaction processing (OLTP) workloads. 
It is best used for traditional data warehousing tasks. 

what is hive engine execution?
Hive is bunch of jars, which jars convert SQL's statements into MapReduce jobs by using YARN framework.
if hive want to execute, it will check
	1) syntax of sql statement
	2) sql statements converted to MapReduce jobs
	3) it will check semantics of job like metadata of tables and columns and job will be executed.

Components of Hive
 1)  Metastore
	It stores metadata for each of the tables like their schema and location. Hive also includes the partition metadata.
 This helps the driver to track the progress of various data sets distributed over the cluster. It stores the data in a traditional RDBMS format. 
 Hive metadata helps the driver to keep a track of the data and it is highly crucial. Backup server regularly replicates the data which it can retrieve in case of data loss

2)  Driver
	It acts like a controller which receives the HiveQL statements. The driver starts the execution of the statement by creating sessions.
 It monitors the life cycle and progress of the execution. Driver stores the necessary metadata generated during the execution of a HiveQL statement.
 It also acts as a collection point of data or query result obtained after the Reduce operation

3)  Compiler
	It performs the compilation of the HiveQL query. This converts the query to an execution plan. The plan contains the tasks.
 It also contains steps needed to be performed by the MapReduce to get the output as translated by the query. The compiler in Hive converts the query to an Abstract Syntax Tree (AST).
 First, check for compatibility and compile-time errors, then converts the AST to a Directed Acyclic Graph (DAG).
	
4)  Optimizer
	It performs various transformations on the execution plan to provide optimized DAG. 
It aggregates the transformations together, such as converting a pipeline of joins to a single join, for better performance.
 The optimizer can also split the tasks, such as applying a transformation on data before a reduce operation, to provide better performance.
	
5) Executor
	Once compilation and optimization complete, the executor executes the tasks. Executor takes care of pipelining the tasks.
	 
6) CLI, UI, Thrift Server
	CLI (command-line interface) provides a user interface for an external user to interact with Hive.
 Thrift server in Hive allows external clients to interact with Hive over a network, similar to the JDBC or ODBC protocols.
 HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — including Pig and MapReduce — to more easily read and write data on the grid.
 WebHCat provides a service that you can use to run Hadoop MapReduce (or YARN), Pig, Hive jobs or perform Hive metadata operations using an HTTP (REST style) interface.

Database is storage for information which provides naming conflicts for tables, views, partitions, columns and so on. Tables are collection of homogeneous units of data which have schema.
In MapReduce, where intermediate job output is materialized to HDFS, Tez and Spark can avoid replication overhead by writing the intermediate output to local disk.

Hive makes the job easy for performing operations like
	Analysis of huge datasets
	Ad-hoc queries
	Data encapsulation

DDL STATEMENTS:       DML STATEMENTS:
	CREATE                LOAD
	SHOW                  SELECT
	DESCRIBE              INSERT
	USE                   DELETE
	DROP                  UPDATE
	ALTER                 EXPORT
	TRUNCATE              IMPORT 
	
Data in Apache Hive can be categorized into:
	Table
	  |-> Partition
	        |-> Bucket
	
	
Tables: 
1)create
	create table if not exists airlines_data(
						Airline_ID INT,
						Name STRING,
						Alias STRING,
						IATA STRING,
	                                        ICAO STRING,
						Callsign STRING,
						Country STRING,
						Active STRING)
	              ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
                      STORED AS TEXTFILE;
	
In hive they are two types of tables present. 
1) managed tables
        A managed table is stored under the hive.metastore.warehouse.dir path property. 
        BY DEFAULT, DATA IN HDFS /APPS/USER/WAREHOUSE/DATABASENAME.DB/TABLENAME/.
        The default location can be overridden by the location property during table creation.
	If a managed table or partition is dropped, 
	the data and metadata associated with that table or partition are deleted and associated views also unavailable. 
	If the PURGE option is not specified, the data is moved to a trash folder for a defined duration but metadata present.
	
	hive> describe formatted airport_data;
	        # Detailed Table Information 
		Database: default 
		Owner: srinu 
		CreateTime: Sun Aug 19 09:08:03 EDT 2018 
		LastAccessTime: UNKNOWN 
		Retention: 0 
		Location: hdfs://name:9000/user/hive/warehouse/airport_data 
		Table Type: MANAGED_TABLE 
				
2) External Tables:
	To create external table, we use external keyword like below
				
	create external table plane_data(
					Name STRING, 
					IATA STRING, 
					ICAO STRING)
				ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
				 STORED AS TEXTFILE;
				
	load data local inpath '/home/srinu/datasets/airport_data/planes.dat' into table plane_data;
			
	hive>describe formatted plane_data; 
		# Detailed Table Information 
		Database: hivetraining 
		Owner: srinu 
		CreateTime: Wed Sep 26 13:41:52 EDT 2018 
		LastAccessTime: UNKNOWN 
		Retention: 0 
		Location: hdfs://NAME:9000/user/hive/warehouse/hivetraining.db/plane_data 
		Table Type: EXTERNAL_TABLE
			
	
Partitions:
   Hive organizes tables into partitions for grouping same type of data together based on a column or partition key.
   Each table in the hive can have one or more partition keys to identify a particular partition.
   Using partition we can also make it faster to do queries on slices of the data.
	
   They are two types of modes present in partition table.
   1) STRICTMODE
	In this mode, we can have at least one static partition column and N dynamic partition column.

 2)  NON-STRICT MODE		 
	In this mode all are dynamic partitioned columns.
	To enable full dynamic partitioning, we have to set below property to non-strict in hive.

	->Before creating Partitioned table in hive first we set the properties for dynamic partition
		SET hive.exec.dynamic.partition=true;
		SET hive.exec.dynamic.partition.mode=non-strict;
		set hive.exec.dynamic.partition.mode=nonstrcit
		By default, number of partitions allowed for single node is 100 and cluster is 1000. 
		So, if you are running in pseudo distributed mode with single node and you partitioned column cardinality is more than 100, 
		than set the below hive property as well.
		 
		hive> set hive.exec.max.dynamic.partitions.pernode=1000
		In order to load data by partition by dynamic table.
		First create table and one more temp table
		
		CREATE TABLE airport_data2 (
		airport_ID INT, name STRING, City STRING, IATA STRING,
		ICAO STRING, Latitude DECIMAL(5,5), Longitude DECIMAL(5,5),
		Altitude INT, Timezone DECIMAL(3,3), DST STRING,
		Tz_time_zone STRING, Type STRING, Source STRING)
		PARTITIONED BY (COUNTRY STRING)
		ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
		STORED AS TEXTFILE;
		
		INSERT INTO TABLE airport_data2 PARTITION (Country) select airport_ID
                                                                        ,name
									,City
									,IATA
									,Country
									,ICAO
									,Latitude
									,Longitude
									,Altitude
									,Timezone,DST 
									,Tz_time_zone
									,Type
									,Source 
									FROM airport_data1;
		
	
	create table if not exists airlines_data(
						Airline_ID INT,
						Name STRING,
						Alias STRING,
						IATA STRING,
	                                        ICAO STRING,
						Callsign STRING,
						Country STRING,
						Active STRING)
				PARTITIONED BY (Country STRING)
	                        ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
				STORED AS TEXTFILE;
	
 A simple query in Hive reads the entire dataset even if we have where clause filter.
 This becomes a bottleneck for running MapReduce jobs over a large table. We can overcome this issue by implementing partitions in Hive.
 Hive makes it very easy to implement partitions by using the automatic partition scheme when the table is created.
	
 There are two types of Partitioning in  Hive
   1) Static Partitioning
	Usually when loading files (big files) into Hive tables static partitions are preferred.
	That saves your time in loading data compared to dynamic partition. 
	User segregate data by values, not by column, file loaded by creating virtual column (not part of the file) with values.
	If you want to use the Static partition in the hive, then
	set property set hive.mapred.mode = strict This property set by default in hive-site.xml
	Static partition is in Strict Mode.
		
	CREATE TABLE airport_data (airport_ID INT,
				   name STRING,
					City STRING,
					IATA STRING,
					ICAO STRING,
					Latitude DECIMAL(5,5),
					Longitude DECIMAL(5,5),
					Altitude INT,
					Timezone DECIMAL(3,3),
					DST STRING,
					Tz_time_zone STRING,
					Type STRING,
					Source STRING) 
				PARTITIONED BY (COUNTRY STRING)
				ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
				STORED AS TEXTFILE;
		
	load data local inpath "/home/srinu/datasets/airport_data/airports.dat' into table airport_data PARTITION (Country ='USA');
	You should use where clause to use limit in the static partition
		
	load data local inpath '/home/user/Desktop/file/station' into table world;
	You can perform Static partition on Hive Manage table or external table
		
    2) Dynamic Partitioning
	Single insert to partition table is known as a dynamic partition.
	Usually, dynamic partition loads the data from the non-partitioned table.
	Dynamic Partition takes more time in loading data compared to static partition.
	When you have large data stored in a table then the Dynamic partition is suitable.
	If you want to partition a number of columns but you don’t know how many columns then also dynamic partition is suitable.
	Dynamic partition there is no required where clause to use limit.
	we can’t perform alter on the Dynamic partition.
	You can perform dynamic partition on hive external table and managed table.
	If you want to use the Dynamic partition, then set property set hive.mapred.mode = non-strict mode.



BUCKETING:
	In Hive Tables or partition are subdivided into buckets based on the hash function of a column in the table to give extra structure to the data that may be used for more efficient queries.
	It is a process of converting one file into N number of buckets. 
	First set the property before create bucketing table in hive
	set hive.enforce.bucketing =true;
	
	In partition we have no control on number of partitions because number of partitions is equal to number of distinct values of the that column. 
	We use CLUSTERED BY clause to divide the table into buckets.
	Bucketing can be done along with Partitioning on Hive tables and even without partitioning.
	Bucketing uses hash function for data partitioning,
	No of buckets = bucketed column.gethashcode % no of Buckets

	CREATE TABLE bucketed_user(
  		firstname VARCHAR(64),
  		lastname  VARCHAR(64),
  		address   STRING,
  		city VARCHAR(64),
  		state  VARCHAR(64),
  		post      STRING,
  		phone1    VARCHAR(64),
  		phone2    STRING,
  		email     STRING,
  		web       STRING)
	COMMENT ‘A bucketed sorted user table’
  	PARTITIONED BY (country VARCHAR(64))
	CLUSTERED BY (state) SORTED BY (city) INTO 32 BUCKETS
 	STORED AS SEQUENCEFILE;

JOINS:
	1. INNER JOIN
	2. FULL OUTER JOIN
	3. LEFT OUTER JOIN
	4. RIGHT OUTER JOIN









Analytics functions:
+---+----+----------+----------+
| V |RANK|ROW_NUMBER|DENSE_RANK|
+---+----+----------+----------+
| a | 1  | 1        | 1        |
| a | 2  | 1        | 1        |
| a | 3  | 1        | 1        |
| b | 4  | 4        | 2        |
| c | 5  | 5        | 3        |
| c | 6  | 5        | 3        |
| d | 7  | 7        | 4        |
| e | 8  | 8        | 5        |
+---+----+----------+----------+

