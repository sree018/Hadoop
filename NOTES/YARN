Yarn (Yet Another resource Negotiator) is framework which manages cluster resources.
YARN application has the following roles:
            1) Yarn ResourceManager
            2) Yarn Application Master
            3) List of containers running on the node managers.

In local mode the driver and workers are on the machine that started the job.
In client mode the driver is running remotely on a data node and the workers are
running on separate data nodes.

ResourceManager:
  ResourceManager failures meant a total cluster failure, as it was a single point of failure.
  The ResourceManager stores the state of the cluster, such as the metadata of the submitted application, 
  information on cluster resource containers,information on the clusterâ€™s general configurations.
  During the time the ResourceManager is down, the cluster is unavailable, and once it gets restarted, 
  all jobs would need a restart, so the half-completed jobs lose any data and need to be restarted again. 
  
  In short, a restart of the ResourceManager used to restart all the running ApplicationMasters.
          This problem done by two ways.
                    1)One way is by creating an active-passive ResourceManager architecture, so
                      that when one goes down, another becomes active and takes responsibility for the cluster.
                    
                    2) One way is by creating an active-passive ResourceManager architecture, so that when one goes down, another becomes active and takes responsibility for the
                        cluster.
      ResourceManger
          |
          |-> Scheduler : allocating resources to applications
          |->ApplicationsManager : accepting job submissions and restarts the ApplicationMaster in case of any failure.

Scheduler:
    The Scheduler is responsible for allocating resources to the various running applications 
    subject to familiar constraints of capacities, queues etc.
    The Scheduler is pure scheduler in the sense that it performs no monitoring
    or tracking of status for the application. Also, it offers no guarantees about
    restarting failed tasks either due to application failure or hardware failures.
    The Scheduler performs its scheduling function based on the resource requirements of
    the applications. The Scheduler has a pluggable policy which is responsible for
    partitioning the cluster resources among the various queues, applications etc.
    
    There are two types of schedulers in scheduler
            1) Capacity Scheduler
                The CapacityScheduler is designed to allow sharing a
         large cluster while giving each organization a minimum capacity guarantee. The
         central idea is that the available resources in the Hadoop Map-Reduce cluster are
         partitioned among multiple organizations who collectively fund the cluster based on
         computing needs.
            2) Fair Scheduler
                Fair scheduling is a method of assigning resources to
                jobs such that all jobs get, on average, an equal share of resources over time.When
                there is a single job running, that job uses the entire cluster. When other jobs are
                  submitted, tasks slots that free up are assigned to the new jobs.
                  
ApplicationMaster:
-----------------
            The ApplicationsManager is responsible for accepting job-submissions,
negotiating the first container for executing the application specific
ApplicationMaster and provides the service for restarting the ApplicationMaster
container on failure. The per-application ApplicationMaster has the responsibility of
negotiating appropriate resource containers from the Scheduler, tracking their status
and monitoring for progress.
            When the ApplicationMaster fails, the ResourceManager simply starts another
container with a new ApplicationMaster running in it for another application attempt.
It is the responsibility of the new ApplicationMaster to recover the state of the
older ApplicationMaster, and this is possible only when ApplicationMasters persist
their states in the external location so that it can be used for future reference.
ApplicatoinMaster will store their state to persisitant disk thus all the status till
the failure can be recovered.

NodeManager:
-----------
            Each Node Manager tracks the available data processing resources on its slave
node and sends regular reports to the Resource Manager. The processing resources in a
Hadoop cluster are consumed in bite-size pieces called containers. A container is a
collection of all the resources necessary to run an application: CPU cores, memory,
network bandwidth, and disk space
            If a Node Manager fails, the ResourceManager detects this failure using a
time-out (that is, stops receiving the heartbeats from the NodeManager). The
ResourceManager then removes the NodeManager from its pool of available NodeManagers.
It also kills all the containers running on that node & reports the failure to all
running AMs. AMs are then responsible for reacting to node failures, by redoing the
work done by any containers running on that node during the fault.

Container Failures:
-------------------
Container failures will be reported by node manager to Resource manager and
Resource manager informs the same to Application Master. Now Application will restart
the container.

Execution plan in Yarn:
   Step1:
            Yarn Client request JOBID for application from ResourceManager, mean while
output directories created by client in HDFS and copies resources from input
directories from HDFS. Client submit application to ResourceManager.

Step2:
-----
            launch ResourceManager pickups JOBID from scheduler and contacts to NodeManager to
container, for to launch new ApplicationMaster.

Step3:
-----
ApplicationMaster creates an object for book keeping and task management. And
it will monitor resources and status of job progress.

Step4:
-----
ApplicationMaster decides to run job with in a container or communicate with
ResourceManager to request more containers for job(mapper jobs and reducer jobs).
Normally mappers and reducers will run by ResourceManager (RM), RM will create
separate container for mapper and reducer. Uber configuration, will allow to run
mapper and reducers in the same process as the ApplicationMaster (AM). Uber jobs are
jobs that are executed within the MapReduce ApplicationMaster. Rather then
communicate with RM to create the mapper and reducer containers. The AM runs the map
and reduce tasks within its own process and avoided the overhead of launching and
communicate with remote containers

Step5:
-----
If ApplicationMaster needs more containers to run job, it will communicate
with NodeManager then NodeManager will collects all resources from HDFS and execute
the job.

Step6:
------ NodeManager after job completion, intermediate data will cleanup and kill
the containers and gives information about resources to ResourceManager.

Step7:
-----
ApplicationMaster will give information about
ResourceManager and Scheduler update their resources.
job status to Client and


